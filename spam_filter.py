# -*- coding: utf-8 -*-
"""Spam Filter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PfuuhfryCNMqux05b40Pb8GbXYiTB9l5

#                        **NATURAL LANGUAGE PROCESSING: EMAIL SPAM FILTER**

The dataset is a set of tagged messages that have been collected for Email Spam Research. It contains 5,574 messages in English that have been tagged as being ham (legitimate) or spam.
I'm building a model that will help to predicit if a particular message can be classified as spam or ham.

# IMPORT LIBRARIES #
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""# IMPORT DATASET #"""

from google.colab import files
uploaded = files.upload()
import io
spam_df = pd.read_csv(io.BytesIO(uploaded['emails.csv']))

# Visualise first and last five entires
spam_df.head(5)

spam_df.tail(5)

"""The dataset file contains one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v0 contains the raw text.
The categorical data ham and spam labels have been encoded as '0' and '1' respectively.

"""

#Generate descriptive statistics.
spam_df.describe()

"""It includes those that summarize the central tendency, dispersion and shape of a dataset’s distribution , excluding NaN values. The statistics gives us the count , mean , standard deviation along with the 25th to 75th percentile of the observations. """

#Print a concise summary of a DataFrame
spam_df.info()

"""Since none of the values are NULL, we dont need to impute any missing data.

# EXPLORATORY DATA ANALYSIS #

**`Text Statistics Visualizations `**
"""

# Most popular Spam message
spam_df.groupby('spam').describe()

"""This message can later be tokenised to identify the most frequently occuring words in spam message.


"""

# Length of the messages
spam_df['length'] = spam_df['text'].apply(len)
spam_df.head()

"""Here I added an additional length column that will show the length associated with each mail. From this column we realise that in general spam messages have more word count compared to ham messages.

**`Histogram`**
"""

# Number Of Characters Present In Emails
spam_df['length'].plot(bins=100, kind='hist')

"""I used 100 bins in the histograms plot to inspect the underlying frequency distribution (eg. Normal distribution), outliers, skewness, etc. The histogram shows some observations are to the extreme right and different from the rest (i.e. Fat Tail). Hence, these need to be removed for better results.

**`Statistical Analysis for the 'length' column`**
"""

spam_df.length.describe()

# Displaying the longest message (i.e max= 43952)
spam_df[spam_df['length'] == 43952]['text'].iloc[0]

"""Dividing the dataframe into two categories: ham and spam"""

# Legitimate Emails
ham = spam_df[spam_df['spam']==0]
ham

# SPAM Emails
spam = spam_df[spam_df['spam']==1]
spam

# Histograms for Ham and Spam dataframes

spam['length'].plot(bins=60, kind='hist', color='red')

"""The histogram shows that spam messages have an average length of 2000 characters. There is also a Fat Tail on the right side which can be due to 'stopwards' which will be removed during cleaning."""

ham['length'].plot(bins=60, kind='hist',color='green')

"""The average word length is around 1500 words.

"""

#Percentage of spam emails
print( 'Spam percentage =', (len(spam) / len(spam_df) )*100,"%")

#Percentage of ham emails
print( 'Ham percentage =', (len(ham) / len(spam_df) )*100,"%")

"""The percentages tell us that the datapoints in spam to ham ratio is 0.32, so the data is somewhat balanced."""

#Show the counts of observations in each categorical bin using bars
sns.countplot(spam_df['spam'], label = "Count")

"""The plot tells us that around 4500 elements have been classified as ham and 1200 have been classified as spam.

**`Ngram Exploration`**
"""

# Top N-grams Barchart

import seaborn as sns
import numpy as np
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from collections import  Counter

def plot_top_ngrams_barchart(text, n=2):
    stop=set(stopwords.words('english'))

    new= text.str.split()
    new=new.values.tolist()
    corpus=[word for i in new for word in i]

    def _get_top_ngram(corpus, n=None):
        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)
        bag_of_words = vec.transform(corpus)
        sum_words = bag_of_words.sum(axis=0) 
        words_freq = [(word, sum_words[0, idx]) 
                      for word, idx in vec.vocabulary_.items()]
        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
        return words_freq[:10]

    top_n_bigrams=_get_top_ngram(text,n)[:10]
    x,y=map(list,zip(*top_n_bigrams))
    sns.barplot(x=y,y=x)

# Bigram
plot_top_ngrams_barchart(spam_df['text'],2)

# Trigram
plot_top_ngrams_barchart(spam_df['text'],3)

"""Ngrams are simply contiguous sequences of n words. Looking at most frequent n-grams gives a better understanding of the context in which the word was used"""

# Wordcloud

import matplotlib.pyplot as plt
import numpy as np
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer,PorterStemmer
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud, STOPWORDS


def plot_wordcloud(text):
    nltk.download('stopwords')
    stop=set(stopwords.words('english'))

    def _preprocess_text(text):
        corpus=[]
        stem=PorterStemmer()
        lem=WordNetLemmatizer()
        for news in text:
            words=[w for w in word_tokenize(news) if (w not in stop)]

            words=[lem.lemmatize(w) for w in words if len(w)>2]

            corpus.append(words)
        return corpus
    
    corpus=_preprocess_text(text)
    
    wordcloud = WordCloud(
        background_color='white',
        stopwords=set(STOPWORDS),
        max_words=100,
        max_font_size=30, 
        scale=3,
        random_state=1)
    
    wordcloud=wordcloud.generate(str(corpus))

    fig = plt.figure(1, figsize=(12, 12))
    plt.axis('off')
 
    plt.imshow(wordcloud)
    plt.show()

plot_wordcloud(spam_df['text'])

"""**`Sentiment Analysis`**"""

# TextBlob for Sentiment Analysis Barchart

from textblob import TextBlob
import matplotlib.pyplot as plt
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk

def sentiment_vader(text, sid):
    ss = sid.polarity_scores(text)
    ss.pop('compound')
    return max(ss, key=ss.get)

def sentiment_textblob(text):
        x = TextBlob(text).sentiment.polarity
        
        if x<0:
            return 'neg'
        elif x==0:
            return 'neu'
        else:
            return 'pos'

def plot_sentiment_barchart(text, method='TextBlob'):
    if method == 'TextBlob':
        sentiment = text.map(lambda x: sentiment_textblob(x))
    elif method == 'Vader':
        nltk.download('vader_lexicon')
        sid = SentimentIntensityAnalyzer()
        sentiment = text.map(lambda x: sentiment_vader(x, sid=sid))
    else:
        raise ValueError('Textblob or Vader')
    
    plt.bar(sentiment.value_counts().index,
            sentiment.value_counts())

plot_sentiment_barchart(spam_df['text'], method='Vader')

"""Since the text is actually email content, the majority is classified as neutral.

**`Named Entity Recognition`**
"""

# Named Entity Barchart

import spacy
from collections import  Counter
import seaborn as sns

def plot_named_entity_barchart(text):
    nlp = spacy.load("en_core_web_sm")
    
    def _get_ner(text):
        doc=nlp(text)
        return [X.label_ for X in doc.ents]
    
    ent=text.apply(lambda x : _get_ner(x))
    ent=[x for sub in ent for x in sub]
    counter=Counter(ent)
    count=counter.most_common()
    
    x,y=map(list,zip(*count))
    sns.barplot(x=y,y=x)

plot_named_entity_barchart(spam_df['text'])

"""Using NER we can get great insights about the types of entities present in the given text dataset. Named entity recognition is an information extraction method in which entities that are present in the text are classified into predefined entity types like “Person”,” Place”,” Organization”, etc.

# NATURAL LANGUAGE PROCESSING #
"""

# IMPORTING LIBRARIES FOR REMOVING PUNCTUATION
import string
string.punctuation

# IMPORTING LIBRARIES FOR REMOVING STOPWORDS
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords.words('english')

# Plot Top Stopwords
stop=set(stopwords.words('english'))
corpus=[]
new= spam_df['text'].str.split()
new=new.values.tolist()
corpus=[word for i in new for word in i]

from collections import defaultdict
dic=defaultdict(int)
for word in corpus:
    if word in stop:
        dic[word]+=1
top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] 
x,y=zip(*top)
plt.bar(x,y)

"""We can evidently see that stopwords such as “the”, ” to” and “and” dominate in emails."""

# Top Non-Stopwords Barchart

import seaborn as sns
from nltk.corpus import stopwords
from collections import  Counter

def plot_top_non_stopwords_barchart(text):
    stop=set(stopwords.words('english'))
    
    new= text.str.split()
    new=new.values.tolist()
    corpus=[word for i in new for word in i]

    counter=Counter(corpus)
    most=counter.most_common()
    x, y=[], []
    for word,count in most[:40]:
        if (word not in stop):
            x.append(word)
            y.append(count)
            
    sns.barplot(x=y,y=x)
plot_top_non_stopwords_barchart(spam_df['text'])

"""**` Pipeline To Clean Up All The Messages `**"""

# The pipeline performs the following:
# (1) remove punctuation
# (2) remove stopwords

def message_cleaning(message):
    Test_punc_removed = [char for char in message if char not in string.punctuation] 
    Test_punc_removed_join = ''.join(Test_punc_removed)
    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]
    return Test_punc_removed_join_clean

# Testing the newly added function
spam_df_clean = spam_df['text'].apply(message_cleaning)

# Email Before Cleaning
print(spam_df['text'][0])

# Email After Cleaning
print(spam_df_clean[0])

#COUNT VECTORIZER
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(analyzer = message_cleaning)
spamham_countvectorizer = vectorizer.fit_transform(spam_df['text'])

"""Count Vectorizer is used to convert all the words within the data to a matrix which shows the frequency of occurence of various words. It is used for interpretation and classification of emotions within text data. It helps in classifying words as having positive or negative connotation."""

print(vectorizer.get_feature_names())

print(spamham_countvectorizer.toarray())

spamham_countvectorizer.shape

"""We've 5728 messages with 37229 columns/words.

# TRAINING THE MODEL USING THE ENTIRE DATASET #
"""

#MULTINOMIAL NAIVE BAYES CLASSIFIER
from sklearn.naive_bayes import MultinomialNB
NB_classifier = MultinomialNB()
label = spam_df['spam'].values
NB_classifier.fit(spamham_countvectorizer, label)

# Create sample texts for testing purpose
testing_sample = ['Free money!!!',"Hello, I am Irtiqa, I would like to book a hotel in SF by January 24th","Hi Kim, Please let me know if you need any further information. Thanks", "You've won a lottery!!"]
# Here the first and last mail should be classified as spam (1) and the rest as ham (0)

# Transform the raw data using countvectorizer
testing_sample_countvectorizer = vectorizer.transform(testing_sample)

# TESTING 
test_predict = NB_classifier.predict(testing_sample_countvectorizer)
test_predict

"""# DIVIDE THE DATA INTO TRAINING AND TESTING PRIOR TO TRAINING #"""

X = spamham_countvectorizer #input
y = label #output

X.shape

y.shape

# Divide the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# TRAINING THE MODEL
from sklearn.naive_bayes import MultinomialNB
NB_classifier = MultinomialNB()
NB_classifier.fit(X_train, y_train)

"""# EVALUATING THE MODEL #"""

from sklearn.metrics import classification_report, confusion_matrix

#Confusion Matrix for Training 
y_predict_train = NB_classifier.predict(X_train)
y_predict_train
cm = confusion_matrix(y_train, y_predict_train)
sns.heatmap(cm, annot=True)

"""From the heatmap we can see that our model was able to correctly classify 4600 emails and miss classified only 13 mails during training """

# #Confusion Matrix for Testing
y_predict_test = NB_classifier.predict(X_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot=True)

"""From the heatmap we can see that our model was able to correctly classify 11400 emails and miss classified only 16 mails during training. So the results are pretty good."""

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_predict_test))

#Classification Report
print(classification_report(y_test, y_predict_test))

"""# ADDITIONAL FEATURE TEXT MINING (TF-IDF) #

TF-IDF stands for "Term Frequency–Inverse Document Frequency" and is a numerical statistic used to reflect how important a word is to a document in a collection or corpus of documents. TFIDF is used as a weighting factor during text search processes and text mining.
"""

# Importing Libraries
from sklearn.feature_extraction.text import TfidfTransformer
emails_tfidf = TfidfTransformer().fit_transform(spamham_countvectorizer)
print(emails_tfidf.shape)

# Sparse matrix with all the values of IF-IDF
print(emails_tfidf[:,:])

"""**`Training the model using the additional feature IF-IDF`**"""

X = emails_tfidf
y = label

# Splitting Data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Training 
from sklearn.naive_bayes import MultinomialNB
NB_classifier = MultinomialNB()
NB_classifier.fit(X_train, y_train)

"""**`Evaluating The Model`**"""

# Confusion Matrix
from sklearn.metrics import classification_report, confusion_matrix
y_predict_train = NB_classifier.predict(X_train)
y_predict_train
cm = confusion_matrix(y_train, y_predict_train)
sns.heatmap(cm, annot=True)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_predict_test))

# Classification Report
print(classification_report(y_test, y_predict_test))

"""We had higher accuracy and better results when we trained the model without this Term Frequency–Inverse Document Frequency (TF-IDF) feature extraction technique."""